{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be652ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from cka import IncrementalCKA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6108c0-0eed-4582-82d8-6b34edf61061",
   "metadata": {},
   "source": [
    "The data and model definitions are based on the pytorch's tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1fc08df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# increase batch size from 4\n",
    "batch_size = 256\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e256cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d5daa9-51d4-46eb-8462-5e67c418b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7441f-9482-4bae-a442-17a8a077dd28",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99eefcf5-7e28-4e88-8925-11266d873e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.296\n",
      "[2,   100] loss: 1.910\n",
      "[3,   100] loss: 1.634\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af5000-3472-405b-87aa-f5cd13862f1f",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e91fa0-3536-4305-9fe3-853bf01cc969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 45 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be450dcc-41d6-4a4e-962c-3e3f4f0a549a",
   "metadata": {},
   "source": [
    "## Calculate mini-batched CKA between intermediate representations in the trained network step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062d6e60-ad69-471b-972c-89e92f3a83c0",
   "metadata": {},
   "source": [
    "Suppose we have two matrices:\n",
    "1. `X`: intermediate representations in $ \\mathbb{R}^{n \\times p_1}$,\n",
    "2. `Y`: intermediate representations in $ \\mathbb{R}^{n \\times p_2}$,\n",
    "\n",
    "where $n$ is the number of samples in a mini-batch, and $p_1$ and $p_2$ are the dimesionality of feature representations.\n",
    "\n",
    "To calculate cka score for a pair of intermediate representations, we call `IncrementalCKA.increment_cka_score` with the following arguments. \n",
    "\n",
    "1. `X`\n",
    "1. `Y`\n",
    "1. the index of layer to compute `X` in $[0, \\ldots, \\texttt{num_representations}]$\n",
    "1. the index of layer to compute `Y` in $[0, \\ldots, \\texttt{num_representations}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18454acd-404e-4a5d-b101-202d9d2e9e85",
   "metadata": {},
   "source": [
    "Recall that we are going to calculate similarity between representations in the following model:\n",
    "\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "```\n",
    "\n",
    "Concretely, we use the following representations:\n",
    "\n",
    "- after every conv2d\n",
    "- after every linear 2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d588739-cc42-4efa-a9b5-629b417b9a7b",
   "metadata": {},
   "source": [
    "`IncrementalCKA` computes cka value given a pair of two intermediate representations,\n",
    "so we need to specify the number of representations (or mightbe the number of intermediate layers) for initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "836184c0-ec42-4f82-8f8b-f86233ed40a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num representations: 5\n"
     ]
    }
   ],
   "source": [
    "num_representations = 5\n",
    "print(\"num representations:\", num_representations)\n",
    "\n",
    "incremental_cka = IncrementalCKA(num_representations, num_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd66ad5-b081-4cd3-9221-26a1a1204d90",
   "metadata": {},
   "source": [
    "To get intermediate value, this notebook uses [`register_module_forward_hook`](https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37e5286-131e-486c-bee2-88e67b54ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_flatten_numpy(tensor):\n",
    "    \n",
    "    return tensor.flatten(start_dim=1).numpy()\n",
    "\n",
    "def get_intermediate(module, input, outputs):\n",
    "    \n",
    "    global intermediates\n",
    "    intermediates.append(tensor_to_flatten_numpy(outputs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e3a38a-d3c7-4723-9164-63bad3f59509",
   "metadata": {},
   "outputs": [],
   "source": [
    "handlers = []\n",
    "\n",
    "handlers.append(net.conv1.register_forward_hook(get_intermediate))\n",
    "handlers.append(net.conv2.register_forward_hook(get_intermediate))\n",
    "handlers.append(net.fc1.register_forward_hook(get_intermediate))\n",
    "handlers.append(net.fc2.register_forward_hook(get_intermediate))\n",
    "handlers.append(net.fc3.register_forward_hook(get_intermediate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba76f95-d307-4062-be2d-e844dc9ef90f",
   "metadata": {},
   "source": [
    "By calling forward pass with the hook above, `intermediates` stores each intermediate representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db935f15-1368-4075-bc9f-136b6b7f9348",
   "metadata": {},
   "source": [
    "Due to the randomness of mini-batch, we evalualte the whole test loop $10$ times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e25a7-95a3-4ae0-9822-08e5d0b26008",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_iters):\n",
    "\n",
    "        for data in testloader:\n",
    "            \n",
    "            intermediates = [] # reset intermediate values\n",
    "            \n",
    "            images, _ = data\n",
    "            net(images)  # call forward pass and stores intermediate values\n",
    "            \n",
    "            # calculate cka for all pairs of representations\n",
    "            for index_x, X in enumerate(intermediates):\n",
    "                for index_y, Y in enumerate(intermediates):\n",
    "                    incremental_cka.increment_cka_score(index_x, index_y, X, Y\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab112ab-273a-4e29-9e03-f76c98d06813",
   "metadata": {},
   "source": [
    "## visualisation\n",
    "\n",
    "To access the cka values, we just call `cka` method. `cka` returns a 2d ndarray whose elemet is cka value between two intermediate representations. To unify the appearence with the original cka paper, we need to reverse the ordering of column and its lables as follows:\n",
    "If you would like to plot several plots with the same heatmap scale for comparision, we recommend specifying the same `vmin` value across different heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08dde5b0-a083-409a-bb8d-9497e1f9824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAARjElEQVR4nO3df6xkdXnH8feHZbFaTInFKuxCQV2x9Aeo62KjVcSqCxpJE03BKi3FbknEatJEaJvWGNOmrdGqKbrZIDWkLaRW0m51KyGtaFtFF1ukLIiuaOC6UEqxWn9EuPc+/WNmdbi5d2bu3plzZo7vFznhzpwz3/MMm33uw3O+53xTVUiSmnFU2wFI0g8Tk64kNcikK0kNMulKUoNMupLUIJOuJDXIpCtJa0hydZIHkty+xv4keW+Sg0luS/KsUWOadCVpbR8Edg7Zfy6wrb/tAt4/akCTriStoao+CTw05JDzgWuq52bguCQnDBvz6EkGuJq3nHJhJ295+92fv7/tECbuMa89v+0QpuKobdvbDmHijnrCiW2HMBWbj39KNjrGIw/ePXbOOeaJT/1NehXqYXuqas86TrcFuHfg9UL/vfvW+sDUk64kNWp5aexD+wl2PUl2pdV+SQxN+iZdSd1Sy02ebQE4aeD1VuDQsA/Y05XULcvL428btxe4qD+L4bnAN6pqzdYCWOlK6piaYKWb5FrgbOD4JAvAW4HNvfPUbmAfcB5wEPgOcPGoMU26krplaXFiQ1XVhSP2F/CG9Yxp0pXULeu4kNYGk66kbmn2Qtq6mXQldctkLpBNjUlXUqdM8kLaNJh0JXWLla4kNWjpkbYjGMqkK6lbbC9IUoNsL0hSg6x0JalBVrqS1Jxa9kKaJDXHSleSGmRPV5Ia5ANvJKlBM17prnvliCTXTCMQSZqIZleOWLehlW6SvSvfAl6U5DiAqnrlGp/bRX+FzZc+YTtnPP5pG49UksYxwYeYT8Oo9sJW4A7gKnorXAbYDrxz2IcGV9js6hLskmbUjM9eGNVe2A58Dvg9eguu3QR8t6o+UVWfmHZwkrReVUtjb20YWulW78GUf5bkQ/1//9eoz0hSq2a80h0rgVbVAvDqJC8HvjndkCRpA2Z89sK6qtaq+ijw0SnFIkkb14VKV5LmxpzPXpCk+dKl9oIkzTzbC5LUIJOuJDXI9oIkNcgLaZLUINsLktQg2wuS1CArXUlqkElXkhpUs/00WZOupG5ZdPaCJDVnxi+krXuNNEmaaRNcIy3JziR3JTmY5IpV9v9Ykn9I8vkkB5JcPGpMk66kbqkafxsiySbgSuBc4HTgwiSnrzjsDcAdVXUGcDbwziTHDBvX9oKkbpnc7IUdwMGquhsgyXXA+fTWjTysgMcnCXAs8BAwtKk89aR72uKmaZ+iFcc8+6lthzBxR23b3nYIU3HUE05sO4SJW37oUNshTMfxT9n4GOtIuoMrl/ft6S+sC7AFuHdg3wJw1ooh/hzYCxwCHg/8cn+ZszVZ6UrqlFoaf8HJwZXLV5HVPrLi9cuAW4FzgKcCNyb5l6pac1kze7qSumVyF9IWgJMGXm+lV9EOuhi4vnoOAl8BnjFsUJOupG6p5fG34fYD25Kc2r84dgG9VsKge4AXAyR5EnAacPewQW0vSOqW5cnckVZVi0kuA24ANgFXV9WBJJf29+8G3g58MMl/0mtHXF5VDw4b16QrqVsm+OyFqtoH7Fvx3u6Bnw8BL13PmCZdSd2yjgtpbTDpSuoWnzImSQ2aUE93Wky6krplxh94Y9KV1C1WupLUnLKnK0kNcvaCJDXI9oIkNcj2giQ1yEpXkhrklDFJapCVriQ1pxadvSBJzbHSlaQGzXtPN8kOoKpqf3/54Z3AF/rPmZSk2TLPlW6St9Jb8/3oJDfSWwnzJuCKJM+sqj9c43PfX2Hzdcft4IU/um2iQUvSWmqeky7wKuBM4DHA/cDWqvpmkncAnwFWTbqDK2x+YOtrZ/u/gKRumfMLaYtVtQR8J8mXDy8rXFXfTTLbjRNJP5zmvNJ9OMnjquo7wLMPv5nkxwCTrqTZM+dJ9wVV9T2AqkddEtwM/OrUopKkI1Q1x0n3cMJd5f0HgaHLDEtSK+a80pWk+WLSlaTm1OJsX24y6UrqltnOuSZdSd0y7zdHSNJ8MelKUoNsL0hSc2wvSFKDatGkK0nNsb0gSc2Z8WeYm3QldYxJV5KaM+uV7lFtByBJk1SL42+jJNmZ5K4kB5NcscYxZye5NcmBJJ8YNaaVrqROmVSlm2QTcCXwEmAB2J9kb1XdMXDMccD7gJ1VdU+Snxg1rpWupE6p5fG3EXYAB6vq7qp6GLgOOH/FMa8Brq+qewCq6oFRg0690n3Jk++b9ilakR2vazuEiTvqCSe2HcJULD90qO0QJm75S7e0HcJ0PP35Gx+jMvahg4vo9u3pr/EIsAW4d2DfAr3FeQc9Hdic5Cbg8cB7quqaYee0vSCpU9bTXhhcRHcVq2XvlXdeHE1vKbMXA48FPp3k5qr64lrnNOlK6pRaHr/SHWEBOGng9VZg5f82LQAPVtW3gW8n+SRwBrBm0rWnK6lTlpcy9jbCfmBbklOTHANcAOxdcczfA7+Q5Ogkj6PXfrhz2KBWupI6ZVKzF6pqMcllwA3AJuDqqjqQ5NL+/t1VdWeSjwG30bst46qqun3YuCZdSZ0ywfYCVbUP2Lfivd0rXr8DeMe4Y5p0JXXKjK/AbtKV1C2TrHSnwaQrqVPGuEDWKpOupE6x0pWkBtU67khrg0lXUqfM+qMdTbqSOmXZSleSmmN7QZIa5OwFSWqQsxckqUH2dCWpQfZ0JalBPntBkhpke0GSGrQ84xfSRq4ckeQZSV6c5NgV7++cXliSdGSWK2NvbRiadJP8Fr3lKN4I3J5kcPnhPxryuV1Jbklyy1//99cmE6kkjaEqY29tGNVe+A3g2VX1rSSnAH+b5JSqeg+rr5QJPHqFzXu2v3jG29qSumTee7qbqupbAFX11SRn00u8P8mQpCtJbZn1Km9UT/f+JGceftFPwK8Ajgd+dopxSdIRWVo+auytDaPOehFw/+AbVbVYVRcBL5haVJJ0hJbXsbVhaHuhqhaG7Pu3yYcjSRtTM975dJ6upE5ZnvGmrklXUqcsW+lKUnNsL0hSg5ZMupLUnBlfl9KkK6lbTLqS1CB7upLUoBl/sqNJV1K3OGVMkhq01HYAI5h0JXXKcqx0JakxM34XsElXUrfM+pSxdh4oKUlTspzxt1GS7ExyV5KDSa4YctxzkiwledWoMa10JXXKpG4DTrIJuBJ4CbAA7E+yt6ruWOW4PwFuGGdcK11JnTLBSncHcLCq7q6qh4HrgPNXOe6NwIeBB8aJb+qV7vGvPnnap2jF0ad3b+GMxTs+2XYIU1Gf/de2Q5i4hz/35bZDmIrHvvzNGx5jPT3dJLuAXQNv7ekvrAuwBbh3YN8CcNaKz28Bfgk4B3jOOOe0vSCpU9Yze2Fw5fJVrFYLrxz+3cDlVbWUMaeqmXQldcoEbwNeAE4aeL0VOLTimO3Adf2EezxwXpLFqvq7tQY16UrqlAlOGdsPbEtyKvA14ALgNYMHVNWph39O8kHgI8MSLph0JXXM0oQq3apaTHIZvVkJm4Crq+pAkkv7+3cfybgmXUmdMsmbI6pqH7BvxXurJtuq+rVxxjTpSuqUWb8jzaQrqVN89oIkNciHmEtSg2wvSFKDfIi5JDXI9oIkNcj2giQ1yNkLktSg5RlPuyZdSZ3ihTRJapA9XUlqkLMXJKlB9nQlqUGznXJNupI6xp6uJDVoacZr3SNegj3JxUP27UpyS5Jbrv7MXUd6Cklat+V1bG044qQLvG2tHVW1p6q2V9X2Xz/rtA2cQpLWZ5kae2vD0PZCktvW2gU8afLhSNLGzHZzYXRP90nAy4Cvr3g/wKemEpEkbcC8X0j7CHBsVd26ckeSm6YRkCRtxKxfSBuadKvqkiH7XrPWPklqizdHSFKDZjvlmnQldYyVriQ1aN4vpEnSXCkrXUlqzlzPXpCkeWN7QZIatFxWupLUmNlOuSZdSR3jlDFJapCzFySpQYsmXUlqzqxXuht5iLkkzZxJrhyRZGeSu5IcTHLFKvt/Jclt/e1TSc4YNaaVrqROqQlNGUuyCbgSeAmwAOxPsreq7hg47CvAC6vq60nOBfYAZw0b16QrqVMmOHthB3Cwqu4GSHIdcD7w/aRbVYOLOdwMbB016NST7uZLfn/ap2jFIx94e9shTNyDH7qn7RCm4sb7T2g7hIm76+gntx3CVPzpBMZYz23ASXYBuwbe2lNVe/o/bwHuHdi3wPAq9hLgH0ed00pXUqesp9LtJ9g9a+zOah9Z9cDkRfSS7vNHndOkK6lTJtXTpVfZnjTweitwaOVBSX4OuAo4t6r+Z9Sgzl6Q1CkTnL2wH9iW5NQkxwAXAHsHD0hyMnA98Lqq+uI48VnpSuqUSc3TrarFJJcBNwCbgKur6kCSS/v7dwN/APw48L4kAItVtX3YuCZdSZ0yyWcvVNU+YN+K93YP/Px64PXrGdOkK6lTlmq2n6hr0pXUKbN+G7BJV1Kn+BBzSWrQbKdck66kjvEh5pLUIJOuJDXI2QuS1CBnL0hSgyb47IWpMOlK6hR7upLUICtdSWrQ0lirn7XHpCupU7wjTZIa5OwFSWrQ3Fe6SZ5BbwXMLfRuaz4E7K2qO6ccmySt26xXukOX60lyOXAdvQXaPktv+YoA1ya5YsjndiW5JcktV11z7STjlaShlqvG3towqtK9BPjpqnpk8M0k7wIOAH+82ocGV9h85MG7Z/vXjqROmfXbgEctTLkMnLjK+ycw1rpuktSsWsc/bRhV6b4Z+KckXwLu7b93MvA04LIpxiVJR6RmvNIdmnSr6mNJng7soHchLfTWgt9fVUsNxCdJ6zL3twFX79fGzQ3EIkkb5m3AktSgua90JWmeLC3PcU9XkubNrN8cYdKV1Cn2dCWpQfZ0JalBVrqS1CAvpElSg2wvSFKDbC9IUoPm/iHmkjRPnKcrSQ2y0pWkBi3P+KMdRz3EXJLmSlWNvY2SZGeSu5IcXG2JsvS8t7//tiTPGjWmSVdSp0wq6SbZBFwJnAucDlyY5PQVh50LbOtvu4D3j4rPpCupU2od2wg7gINVdXdVPUxvkd7zVxxzPnBN9dwMHJfkhGGDTr2nu/n4p2Ta5zgsya7+ophTt/nyv2jiNEBz3+vky6d9hh9o8s/qkiZO0tfk92rKvH2nxYe/NnbOSbKLXoV62J6B77qFHyxTBr1Vc85aMcRqx2wB7lvrnF2rdHeNPmQudfF7dfE7QTe/Vxe/E9Bbubyqtg9sg79cVkveKwvkcY55lK4lXUmalAXgpIHXW4FDR3DMo5h0JWl1+4FtSU5NcgxwAbB3xTF7gYv6sxieC3yjqtZsLUD35unOTd9pnbr4vbr4naCb36uL32mkqlpMchlwA7AJuLqqDiS5tL9/N7APOA84CHwHuHjUuJn1h0NIUpfYXpCkBpl0JalBnUi6o27Vm0dJrk7yQJLb245lkpKclOTjSe5MciDJm9qOaaOS/EiSzyb5fP87va3tmCYpyaYk/5HkI23H0gVzn3THvFVvHn0Q2Nl2EFOwCPx2Vf0U8FzgDR348/oecE5VnQGcCezsX8nuijcBd7YdRFfMfdJlvFv15k5VfRJ4qO04Jq2q7quqf+///H/0/jJvaTeqjenfAvqt/svN/a0TV6iTbAVeDlzVdixd0YWku9ZteJpxSU4Bngl8puVQNqz/v+C3Ag8AN1bV3H+nvncDbwFm+3mJc6QLSXfdt+GpfUmOBT4MvLmqvtl2PBtVVUtVdSa9O5J2JPmZlkPasCSvAB6oqs+1HUuXdCHprvs2PLUryWZ6Cfevqur6tuOZpKr6X+AmutGPfx7wyiRfpde2OyfJX7Yb0vzrQtId51Y9zYgkAT4A3FlV72o7nklI8sQkx/V/fizwi8AXWg1qAqrqd6pqa1WdQu/v1T9X1WtbDmvuzX3SrapF4PCtencCf1NVB9qNauOSXAt8GjgtyUKSJp9QOE3PA15Hr2q6tb+d13ZQG3QC8PEkt9ErAm6sKqdXaVXeBixJDZr7SleS5olJV5IaZNKVpAaZdCWpQSZdSWqQSVeSGmTSlaQG/T/WgWyURzhiswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(incremental_cka.cka()[::-1], yticklabels=range(num_representations-1, -1, -1), vmin=0., vmax=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcefb34-6807-4771-a60c-6ffde242e087",
   "metadata": {},
   "source": [
    "### Post-processing\n",
    "\n",
    "If you re-train the model, remove `register_forward_hook` by calling `remove` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e4f824-551d-4ae1-b3cb-adc953225a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for handler in handlers:\n",
    "    handler.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb27a9b-76b5-4f86-be18-8aa7b5ecfd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
